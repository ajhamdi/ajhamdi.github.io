[{"authors":["admin"],"categories":null,"content":"I am a postdoctoral research fellow in machine learning and computer vision at the Visual Geomtry Group of the University of Oxford with Prof. Andrew Zisserman and Prof. Andrea Vedaldi, and a research felllow (JRF) at Kellogg College. The postdoc is supported by The Ibn Rushd Postdoctoral Fellowship Awarded. Prior to that, I earned PhD and MS degrees in Electrical an computer Engineering (Computer Vision) from KAUST, working on 3D understanding with deep neural networks. I was part of the Image and Video Understanding Laboratory (IVUL) in the Visual Computing Center (VCC), advised by Bernard Ghanem. I was a visiting PhD student with Prof Matthias Niessner at TUM in 2022 for 5 months in Munich. I received my my undergraduate degree form KFUPM in Electrical Engineering. My carear goal is to develop robust deep learning tools for 3D understanding and creation and to expand the access of AI to disadvantaged groups in the Arabic region. [video, article, TedX talk]. I am also the founder and president of fihm.ai (biggest Arabic online AI platform).\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://abdullahamdi.com/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a postdoctoral research fellow in machine learning and computer vision at the Visual Geomtry Group of the University of Oxford with Prof. Andrew Zisserman and Prof. Andrea Vedaldi, and a research felllow (JRF) at Kellogg College. The postdoc is supported by The Ibn Rushd Postdoctoral Fellowship Awarded. Prior to that, I earned PhD and MS degrees in Electrical an computer Engineering (Computer Vision) from KAUST, working on 3D understanding with deep neural networks.","tags":null,"title":"Abdullah Hamdi","type":"authors"},{"authors":["Emmanuelle Bourigault*","**Abdullah Hamdi***","Amir Jamaludin"],"categories":null,"content":"","date":1758056400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1758056400,"objectID":"007838d145a7f271b9e076693d428f33","permalink":"https://abdullahamdi.com/publication/xdiff/","publishdate":"2025-09-17T00:00:00+03:00","relpermalink":"/publication/xdiff/","section":"publication","summary":"Magnetic Resonance Imaging (MRI) is a crucial diagnostic tool, but high-resolution scans are often slow and expensive due to extensive data acquisition requirements. Traditional MRI reconstruction methods aim to expedite this process by filling in missing frequency components in the K-space, performing 3D-to-3D reconstructions that demand full 3D scans. In contrast, we introduce X-Diffusion, a novel cross-sectional diffusion model that reconstructs detailed 3D MRI volumes from extremely sparse spatial-domain inputs, achieving 2D-to-3D reconstruction from as little as a single 2D MRI slice or few slices. A key aspect of X-Diffusion is that it models MRI data as holistic 3D volumes during the cross-sectional training and inference, unlike previous learning approaches that treat MRI scans as collections of 2D slices in standard planes (coronal, axial, sagittal). We evaluated X-Diffusion on brain tumor MRIs from the BRATS dataset and full-body MRIs from the UK Biobank dataset. Our results demonstrate that X-Diffusion not only surpasses state-of-the-art methods in quantitative accuracy (PSNR) on unseen data but also preserves critical anatomical features such as tumor profiles, spine curvature, and brain volume. Remarkably, the model generalizes beyond the training domain, successfully reconstructing knee MRIs despite being trained exclusively on brain data. Medical expert evaluations further confirm the clinical relevance and fidelity of the generated images. To our knowledge, X-Diffusion is the first method capable of producing detailed 3D MRIs from highly limited 2D input data, potentially accelerating MRI acquisition and reducing associated costs.","tags":["Medical Imaing","MRI","multi-view","novel view synthesis","3D","Diffusion"],"title":"X-Diffusion: Generating Detailed 3D MRI Volumes From a Single Image Using Cross-Sectional Diffusion Models","type":"publication"},{"authors":["Emmanuelle Bourigault","Amir Jamaludin","**Abdullah Hamdi**"],"categories":null,"content":"","date":1757451600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1757451600,"objectID":"3de54b934f70a85cd3c1adc4b3eae77d","permalink":"https://abdullahamdi.com/publication/ukbob/","publishdate":"2025-09-10T00:00:00+03:00","relpermalink":"/publication/ukbob/","section":"publication","summary":"In medical imaging, the primary challenge is collecting large-scale labeled data due to privacy concerns, logistics, and high labeling costs. In this work, we present the UK Biobank Organs and Bones (UKBOB), the largest labeled dataset of body organs, comprising 51,761 MRI 3D samples (equivalent to 17.9 million 2D images) and more than 1.37 billion 2D segmentation masks of 72 organs, all based on the UK Biobank MRI dataset. We utilize automatic labeling, introduce an automated label cleaning pipeline with organ-specific filters, and manually annotate a subset of 300 MRIs with 11 abdominal classes to validate the quality (referred to as UKBOB-manual). This approach allows for scaling up the dataset collection while maintaining confidence in the labels. We further confirm the validity of the labels by demonstrating zero-shot generalization of trained models on the filtered UKBOB to other small labeled datasets from similar domains (e.g., abdominal MRI). To further mitigate the effect of noisy labels, we propose a novel method called Entropy Test-time Adaptation (ETTA) to refine the segmentation output. We use UKBOB to train a foundation model, Swin-BOB, for 3D medical image segmentation based on the Swin-UNetr architecture, achieving state-of-the-art results in several benchmarks in 3D medical imaging, including the BRATS brain MRI tumor challenge (with a 0.4% improvement) and the BTCV abdominal CT scan benchmark (with a 1.3% improvement). The pre-trained models and the code are available at the project website, and the filtered labels will be made available with the UK Biobank.","tags":["Medical Imaing","MRI","multi-view","novel view synthesis","3D","Diffusion","3D segmentation","medical image segmentation"],"title":"UKBOB: One Billion MRI Labeled Masks for Generalizable 3D Medical Image Segmentation","type":"publication"},{"authors":["Wenxuan Zhu","Bing Li","Cheng Zheng","Jinjie Mai","Jun Chen","Letian Jiang","**Abdullah Hamdi**","Sara Rojas Martinez","Chia-Wen Lin","Mohamed Elhoseiny","Bernard Ghanem"],"categories":null,"content":"","date":1754773200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1754773200,"objectID":"ed8e47fb2b77b1e2a3050349e7e1e9c1","permalink":"https://abdullahamdi.com/publication/bench4d/","publishdate":"2025-08-10T00:00:00+03:00","relpermalink":"/publication/bench4d/","section":"publication","summary":"Multimodal Large Language Models (MLLMs) have demonstrated impressive 2D image/video understanding capabilities.However, there are no publicly standardized benchmarks to assess the abilities of MLLMs in understanding the 4D objects.In this paper, we introduce 4D-Bench, the first benchmark to evaluate the capabilities of MLLMs in 4D object understanding, featuring tasks in 4D object Question Answering (4D object QA) and 4D object captioning.4D-Bench provides 4D objects with diverse categories, high-quality annotations, and tasks necessitating multi-view spatial-temporal understanding, different from existing 2D image/video-based benchmarks.With 4D-Bench, we evaluate a wide range of open-source and closed-source MLLMs.The results from the 4D object captioning experiment indicate that MLLMs generally exhibit weaker temporal understanding compared to their appearance understanding, notably, while open-source models approach closed-source performance in appearance understanding, they show larger performance gaps in temporal understanding.4D object QA yields surprising findings: even with simple single-object videos, MLLMs perform poorly, with state-of-the-art GPT-4o achieving only 63% accuracy compared to the human baseline of 91%.These findings highlight a substantial gap in 4D object understanding and the need for further advancements in MLLMs.","tags":["Multimodal Large Language Models","MLLMs","multi-view","novel view synthesis","3D","4D","4D Understanding","dataset","benchmark"],"title":"4D-Bench: Benchmarking Multi-modal Large Language Models for 4D Object Understanding","type":"publication"},{"authors":["Bingchen Gong","Diego Gomez","**Abdullah Hamdi**","Abdelrahman Eldesokey","Ahmed Abdelreheem","Peter Wonka","Maks Ovsjanikov"],"categories":null,"content":"","date":1754773200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1754773200,"objectID":"fbd3663e281258e7f80b0cf8f2aaeb9f","permalink":"https://abdullahamdi.com/publication/zerokey/","publishdate":"2025-08-10T00:00:00+03:00","relpermalink":"/publication/zerokey/","section":"publication","summary":"We propose a novel zero-shot approach for keypoint detection on 3D shapes. Point-level reasoning on visual data is challenging as it requires precise localization capability, posing problems even for powerful models like DINO or CLIP. Traditional methods for 3D keypoint detection rely heavily on annotated 3D datasets and extensive supervised training, limiting their scalability and applicability to new categories or domains. In contrast, our method utilizes the rich knowledge embedded within Multi-Modal Large Language Models (MLLMs). Specifically, we demonstrate, for the first time, that pixel-level annotations used to train recent MLLMs can be exploited for both extracting and naming salient keypoints on 3D models without any ground truth labels or supervision. Experimental evaluations demonstrate that our approach achieves competitive performance on standard benchmarks compared to supervised methods, despite not requiring any 3D keypoint annotations during training. Our results highlight the potential of integrating language models for localized 3D shape understanding. This work opens new avenues for cross-modal learning and underscores the effectiveness of MLLMs in contributing to 3D computer vision challenges.","tags":["Large Language Models","MRI","multi-view","3D Key Point","3D","LLM","3D Detection"],"title":"ZeroKey: Point-Level Reasoning and Zero-Shot 3D Keypoint Detection from Large Language Models","type":"publication"},{"authors":["Jiayuan Zhu","**Abdullah Hamdi**","Yunli Qi","Yueming Jin","Junde Wu"],"categories":null,"content":"","date":1732222800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1732222800,"objectID":"873368dac03586db318138ecd06da4f9","permalink":"https://abdullahamdi.com/publication/medsam/","publishdate":"2024-11-22T00:00:00+03:00","relpermalink":"/publication/medsam/","section":"publication","summary":"Medical image segmentation plays a pivotal role in clinical diagnostics and treatment planning, yet existing models often face challenges in generalization and in handling both 2D and 3D data uniformly. In this paper, we introduce Medical SAM 2 (MedSAM-2), a generalized auto-tracking model for universal 2D and 3D medical image segmentation. The core concept is to leverage the Segment Anything Model 2 (SAM2) pipeline to treat all 2D and 3D medical segmentation tasks as a video object tracking problem. To put it into practice, we propose a novel self-sorting memory bank mechanism that dynamically selects informative embeddings based on confidence and dissimilarity, regardless of temporal order. This mechanism not only significantly improves performance in 3D medical image segmentation but also unlocks a One-Prompt Segmentation capability for 2D images, allowing segmentation across multiple images from a single prompt without temporal relationships. We evaluated MedSAM-2 on five 2D tasks and nine 3D tasks, including white blood cells, optic cups, retinal vessels, mandibles, coronary arteries, kidney tumors, liver tumors, breast cancer, nasopharynx cancer, vestibular schwannoma, mediastinal lymph nodules, cerebral artery, inferior alveolar nerve, and abdominal organs, comparing it against state-of-the-art (SOTA) models in task-tailored, general and interactive segmentation settings. Our findings demonstrate that MedSAM-2 surpasses a wide range of existing models and updates new SOTA on several benchmarks.","tags":["Medical Imaing","MRI","multi-view","Segment Anything Model","3D","Diffusion","SAM","Medical Imaing Segmentation"],"title":"Medical SAM 2: Segment Medical Images as Video via Segment Anything Model 2","type":"publication"},{"authors":["Jan Held*","Renaud Vandeghen*","**Abdullah Hamdi***","Adrien Deliege,","Anthony Cioppa","Silvio Giancola","Andrea Vedaldi","Bernard Ghanem","Marc Van Droogenbroeck"],"categories":null,"content":"","date":1732050000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1732050000,"objectID":"2766f6f23611625b6ad1bb2408705704","permalink":"https://abdullahamdi.com/publication/cvx/","publishdate":"2024-11-20T00:00:00+03:00","relpermalink":"/publication/cvx/","section":"publication","summary":"Recent advances in radiance field reconstruction, such as 3D Gaussian Splatting (3DGS), have achieved high-quality novel view synthesis and fast rendering by representing scenes with compositions of Gaussian primitives. However, 3D Gaussians present several limitations for scene reconstruction. Accurately capturing hard edges is challenging without significantly increasing the number of Gaussians, creating a large memory footprint. Moreover, they struggle to represent flat surfaces, as they are diffused in space. Without hand-crafted regularizers, they tend to disperse irregularly around the actual surface. To circumvent these issues, we introduce a novel method, named 3D Convex Splatting (3DCS), which leverages 3D smooth convexes as primitives for modeling geometrically-meaningful radiance fields from multi-view images. Smooth convex shapes offer greater flexibility than Gaussians, allowing for a better representation of 3D scenes with hard edges and dense volumes using fewer primitives. Powered by our efficient CUDA-based rasterizer, 3DCS achieves superior performance over 3DGS on benchmarks such as Mip-NeRF360, Tanks and Temples, and Deep Blending. Specifically, our method attains an improvement of up to 0.81 in PSNR and 0.026 in LPIPS compared to 3DGS while maintaining high rendering speeds and reducing the number of required primitives. Our results highlight the potential of 3D Convex Splatting to become the new standard for high-quality scene reconstruction and novel view synthesis.","tags":["3D point cloud","3D radiance fields","multi-view","novel view synthesis","NeRF","Gaussian Splatting","convex splatting","convex"],"title":"3D Convex Splatting: Radiance Field Rendering with 3D Smooth Convexes","type":"publication"},{"authors":["**Abdullah Hamdi**","Faisal AlZahrani","Silvio Giancola","Bernard Ghanem"],"categories":null,"content":"","date":1731272400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1731272400,"objectID":"cc9490a6102bec27a93752ee65d231d4","permalink":"https://abdullahamdi.com/publication/mvtn-ijcv/","publishdate":"2024-11-11T00:00:00+03:00","relpermalink":"/publication/mvtn-ijcv/","section":"publication","summary":"Multi-view projection techniques have shown themselves to be highly effective in achieving top-performing results in the recognition of 3D shapes. These methods involve learning how to combine information from multiple view-points. However, the camera view-points from which these views are obtained are often fixed for all shapes. To overcome the static nature of current multi-view techniques, we propose learning these view-points. Specifically, we introduce the Multi-View Transformation Network (MVTN), which uses differentiable rendering to determine optimal view-points for 3D shape recognition. As a result, MVTN can be trained end-to-end with any multi-view network for 3D shape classification. We integrate MVTN into a novel adaptive multi-view pipeline that is capable of rendering both 3D meshes and point clouds. Our approach demonstrates state-of-the-art performance in 3D classification and shape retrieval on several benchmarks (ModelNet40, ScanObjectNN, ShapeNet Core55). Further analysis indicates that our approach exhibits improved robustness to occlusion compared to other methods. We also investigate additional aspects of MVTN, such as 2D pretraining and its use for segmentation. To support further research in this area, we have released MVTorch, a PyTorch library for 3D understanding and generation using multi-view projections.","tags":["3D point cloud","3D classification","multi-view","3D retrieval"],"title":"MVTN: Learning Multi-view Transformations for 3D Understanding","type":"publication"},{"authors":["Lorenza Prospero","**Abdullah Hamdi**","Joao F. Henriques","Christian Rupprecht"],"categories":null,"content":"","date":1726347600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1726347600,"objectID":"9ca7518640bad4994bbe4d728b91ace4","permalink":"https://abdullahamdi.com/publication/gst/","publishdate":"2024-09-15T00:00:00+03:00","relpermalink":"/publication/gst/","section":"publication","summary":"We base our work on 3D Gaussian Splatting (3DGS), a scene representation composed of a mixture of Gaussians. Predicting such mixtures for a human from a single input image is challenging, as it is a non-uniform density (with a many-to-one relationship with input pixels) with strict physical constraints. At the same time, it needs to be flexible to accommodate a variety of clothes and poses. Our key observation is that the vertices of standardized human meshes (such as SMPL) can provide an adequate density and approximate initial position for Gaussians. We can then train a transformer model to jointly predict comparatively small adjustments to these positions, as well as the other Gaussians' attributes and the SMPL parameters. We show empirically that this combination (using only multi-view supervision) can achieve fast inference of 3D human models from a single image without test-time optimization, expensive diffusion models, or 3D points supervision. We also show that it can improve 3D pose estimation by better fitting human models that account for clothes and other variations.","tags":["Body pose estimation","MRI","multi-view","novel view synthesis","3D","3D Gaussian splatting"],"title":"GST: Precise 3D Human Body from a Single Image with Gaussian Splatting Transformers","type":"publication"},{"authors":["Jinjie Mai","Wenxuan Zhu","Sara Rojas Martinez","Jesus Zarzar","**Abdullah Hamdi**","Guocheng Qian","Bing Li","Silvio Giancola","Bernard Ghanem"],"categories":null,"content":"","date":1724965200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1724965200,"objectID":"42905b19681ee4cfbaeb71d0ed467ea2","permalink":"https://abdullahamdi.com/publication/track/","publishdate":"2024-08-30T00:00:00+03:00","relpermalink":"/publication/track/","section":"publication","summary":"Neural radiance fields (NeRFs) generally require many images with accurate poses for accurate novel view synthesis, which does not reflect realistic setups where views can be sparse and poses can be noisy. Previous solutions for learning NeRFs with sparse views and noisy poses only consider local geometry consistency with pairs of views. Closely following bundle adjustment in Structure-from-Motion (SfM), we introduce TrackNeRF for more globally consistent geometry reconstruction and more accurate pose optimization. TrackNeRF introduces feature tracks, i.e. connected pixel trajectories across all visible views that correspond to the same 3D points. By enforcing reprojection consistency among feature tracks, TrackNeRF encourages holistic 3D consistency explicitly. Through extensive experiments, TrackNeRF sets a new benchmark in noisy and sparse view reconstruction. In particular, TrackNeRF shows significant improvements over the state-of-the-art BARF and SPARF by ∼8 and ∼1 in terms of PSNR on DTU under various sparse and noisy view setups. ","tags":["3D point cloud","3D classification","multi-view","3D retrieval"],"title":"TrackNeRF: Bundle Adjusting NeRF from Sparse and Noisy Views via Feature Tracks","type":"publication"},{"authors":["**Abdullah Hamdi**","Luke Melas-Kyriazi","Jinjie Mai","Guocheng Qian","Ruoshi Liu","Carl Vondrick","Bernard Ghanem","Andrea Vedaldi"],"categories":null,"content":"","date":1708117200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1708117200,"objectID":"5f64480129f744983090706227fc63a7","permalink":"https://abdullahamdi.com/publication/ges/","publishdate":"2024-02-17T00:00:00+03:00","relpermalink":"/publication/ges/","section":"publication","summary":"Advancements in 3D Gaussian Splatting (GS) have significantly accelerated 3D reconstruction and generation. However, it may require a very large number of Gaussians, which can become a substantial memory footprint. This paper introduces GES (Generalized Exponential Splatting), a novel representation that employs Generalized Exponential Function (GEF) to model 3D scenes, requiring far fewer particles to represent a scene and thus significantly outperforming Gaussian Splatting methods in efficiency with a plug-and-play replacement ability for Gaussian-based utilities. GES is validated theoretically and empirically in both principled 1D setup and realistic 3D contexts. It is shown to more accurately represent signals with sharp edges, which are typically challenging for Gaussians due to their inherent low-pass characteristics of the Gaussian function.Our empirical analysis demonstrates that GEF outperforms Gaussians in fitting natural-occurring signals (e.g. squares, triangles, parabolic signals), thereby reducing the need for extensive splitting operations that increase the memory footprint of Gaussian Splatting. With the aid of a frequency-modulated loss, GES achieves competitive performance in novel-view synthesis standard benchmarks while requiring less than half the memory storage of Gaussian Splatting.","tags":["3D point cloud","3D radiance fields","multi-view","novel view synthesis","NeRF","Gaussian Splatting"],"title":"GES: Generalized Exponential Splatting for Efficient Radiance Field Rendering","type":"publication"},{"authors":["Guocheng Qian","Jinjie Mai","**Abdullah Hamdi**","Jian Ren","Aliaksandr Siarohin","Bing Li","Hsin-Ying Lee","Ivan Skorokhodov","Peter Wonka","Sergey Tulyakov","Bernard Ghanem"],"categories":null,"content":"","date":1705266000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1705266000,"objectID":"54e04b3ae123e5b497f14f4bb91765d1","permalink":"https://abdullahamdi.com/publication/magic123/","publishdate":"2024-01-15T00:00:00+03:00","relpermalink":"/publication/magic123/","section":"publication","summary":"We present `Magic123`, a two-stage coarse-to-fine solution for high-quality, textured 3D meshes generation from a single unposed image in the wild using both 2D and 3D priors. In the first stage, we optimize a neural radiance field to produce a coarse geometry. In the second stage, we adopt a memory-efficient differentiable mesh representation to yield a high-resolution mesh with a visually appealing texture. In both stages, the 3D content is learned through reference view supervision and novel views guided by both 2D and 3D diffusion priors. We introduce a single tradeoff parameter between the 2D and 3D priors to control exploration (more imaginative) and exploitation (more precise) of the generated geometry. Additionally, We employ textual inversion and monocular depth regularization to encourage consistent appearances across views and to prevent degenerate solutions, respectively. Magic123 demonstrates a significant improvement over previous image-to-3D techniques, as validated through extensive experiments on synthetic benchmarks and diverse real-world images.","tags":["3D point cloud","3D radiance fields","multi-view","3D retrieval","NeRF","sparse voxels"],"title":"Magic123: One Image to High-Quality 3D Object Generation Using Both 2D and 3D Diffusion Priors","type":"publication"},{"authors":["Guocheng Qian","**Abdullah Hamdi**","Xingdi Zhang","Bernard Ghanem"],"categories":null,"content":"","date":1697403600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1697403600,"objectID":"a721d6e6bf14d4314fab1bbdda21ceb1","permalink":"https://abdullahamdi.com/publication/pix4point/","publishdate":"2023-10-16T00:00:00+03:00","relpermalink":"/publication/pix4point/","section":"publication","summary":"While Transformers have achieved impressive success in natural language processing and computer vision, their performance on 3D point clouds is relatively poor. This is mainly due to the limitation of Transformers: a demanding need for extensive training data. Unfortunately, in the realm of 3D point clouds, the availability of large datasets is a challenge, exacerbating the issue of training Transformers for 3D tasks. In this work, we solve the data issue of point cloud Transformers from two perspectives: (i) introducing more inductive bias to reduce the dependency of Transformers on data, and (ii) relying on cross-modality pretraining. More specifically, we first present Progressive Point Patch Embedding and present a new point cloud Transformer model namely PViT. PViT shares the same backbone as Transformer but is shown to be less hungry for data, enabling Transformer to achieve performance comparable to the state-of-the-art. Second, we formulate a simple yet effective pipeline dubbed Pix4Point that allows harnessing Transformers pretrained in the image domain to enhance downstream point cloud understanding. This is achieved through a modality-agnostic Transformer backbone with the help of a tokenizer and decoder specialized in the different domains. Pretrained on a large number of widely available images, significant gains of PViT are observed in the tasks of 3D point cloud classification, part segmentation, and semantic segmentation on ScanObjectNN, ShapeNetPart, and S3DIS, respectively.","tags":["3D point cloud","3D understanding","transformers","3D segmentation","3D classification"],"title":"Pix4Point: Image Pretrained Standard Transformers for 3D Point Cloud Understanding","type":"publication"},{"authors":["Jinjie Mai","**Abdullah Hamdi**","Silvio Giancola","Chen Zhao","Bernard Ghanem"],"categories":null,"content":"","date":1695589200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1695589200,"objectID":"aab30e84bbceb55b3b9cb2623a428238","permalink":"https://abdullahamdi.com/publication/egoloc-iccv/","publishdate":"2023-09-25T00:00:00+03:00","relpermalink":"/publication/egoloc-iccv/","section":"publication","summary":"With the recent advances in video and 3D understanding, novel 4D spatio-temporal methods fusing both concepts have emerged. Towards this direction, the Ego4D Episodic Memory Benchmark proposed a task for Visual Queries with 3D Localization (VQ3D). Given an egocentric video clip and an image crop depicting a query object, the goal is to localize the 3D position of the center of that query object with respect to the camera pose of a query frame. Current methods tackle the problem of VQ3D by unprojecting the 2D localization results of the sibling task Visual Queries with 2D Localization (VQ2D) into 3D predictions. Yet, we point out that the low number of camera poses caused by camera re-localization from previous VQ3D methods severally hinders their overall success rate. In this work, we formalize a pipeline (we dub EgoLoc) that better entangles 3D multiview geometry with 2D object retrieval from egocentric videos. Our approach involves estimating more robust camera poses and aggregating multi-view 3D displacements by leveraging the 2D detection confidence, which enhances the success rate of object queries and leads to a significant improvement in the VQ3D baseline performance. Specifically, our approach achieves an overall success rate of up to 87.12%, which sets a new state-of-the-art result in the VQ3D task. We provide a comprehensive empirical analysis of the VQ3D task and existing solutions and highlight the remaining challenges in VQ3D. The code and models will be released upon publication to set a new standard for the VQ3D task","tags":["3D point cloud","3D classification","multi-view","3D retrieval"],"title":"EgoLoc: Revisiting 3D Object Localization from Egocentric Videos with Visual Queries","type":"publication"},{"authors":["**Abdullah Hamdi**","Bernard Ghanem","Matthias Nießner"],"categories":null,"content":"","date":1692478800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1692478800,"objectID":"4b268dd6fee6ebf6a4b3e14cf350e123","permalink":"https://abdullahamdi.com/publication/sparf/","publishdate":"2023-08-20T00:00:00+03:00","relpermalink":"/publication/sparf/","section":"publication","summary":"Recent advances in Neural Radiance Fields (NeRFs) treat the problem of novel view synthesis as Sparse Radiance Field (SRF) optimization using sparse voxels for efficient and fast rendering (Plenoxels,InstantNGP). In order to leverage machine learning and adoption of SRFs as a 3D representation, we present SPARF, a large-scale ShapeNet-based synthetic dataset for novel view synthesis consisting of ~ 17 million images rendered from nearly 40,000 shapes at high resolution (400 X 400 pixels). The dataset is orders of magnitude larger than existing synthetic datasets for novel view synthesis and includes more than one million 3D-optimized radiance fields with multiple voxel resolutions. Furthermore, we propose a novel pipeline (SuRFNet) that learns to generate sparse voxel radiance fields from only few views. This is done by using the densely collected SPARF dataset and 3D sparse convolutions. SuRFNet employs partial SRFs from few/one images and a specialized SRF loss to learn to generate high-quality sparse voxel radiance fields that can be rendered from novel views. Our approach achieves state-of-the-art results in the task of unconstrained novel view synthesis based on few views on ShapeNet as compared to recent baselines.","tags":["3D point cloud","3D radiance fields","multi-view","3D retrieval","NeRF","sparse voxels"],"title":"SPARF: Large-Scale Learning of 3D Sparse Radiance Fields from Few Input Images","type":"publication"},{"authors":["Jan Held","Anthony Cioppa","Silvio Giancola","**Abdullah Hamdi**","Bernard Ghanem","Marc Van Droogenbroeck"],"categories":null,"content":"","date":1687381200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1687381200,"objectID":"957aed0b346e9060faf15db2d149b29b","permalink":"https://abdullahamdi.com/publication/vars-cvpr/","publishdate":"2023-06-22T00:00:00+03:00","relpermalink":"/publication/vars-cvpr/","section":"publication","summary":"The Video Assistant Referee (VAR) has revolutionized association football, enabling referees to review incidents on the pitch, make informed decisions, and ensure fairness. However, due to the lack of referees in many countries and the high cost of the VAR infrastructure, only professional leagues can benefit from it. In this paper, we propose a Video Assistant Referee System (VARS) that can automate soccer decision-making. VARS leverages the latest findings in multi-view video analysis, to provide real-time feedback to the referee, and help them make informed decisions that can impact the outcome of a game. To validate VARS, we introduce SoccerNet-MVFoul, a novel video dataset of soccer fouls from multiple camera views, annotated with extensive foul descriptions by a professional soccer referee, and we benchmark our VARS to automatically recognize the characteristics of these fouls. We believe that VARS has the potential to revolutionize soccer refereeing and take the game to new heights of fairness and accuracy across all levels of professional and amateur federations.","tags":["3D point cloud","3D radiance fields","multi-view","3D retrieval","NeRF","sparse voxels"],"title":"VARS: Video Assistant Referee System for Automated Soccer Decision Making From Multiple Views","type":"publication"},{"authors":["**Abdullah Hamdi**","Silvio Giancola","Bernard Ghanem"],"categories":null,"content":"","date":1664053200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664053200,"objectID":"50869c3251b905aeef93bce9f4fc1c33","permalink":"https://abdullahamdi.com/publication/voint-cloud/","publishdate":"2022-09-25T00:00:00+03:00","relpermalink":"/publication/voint-cloud/","section":"publication","summary":"Multi-view projection methods have demonstrated promising performance on 3D understanding tasks like 3D classification and segmentation. However, it remains unclear how to combine such multi-view methods with the widely available 3D point clouds. Previous methods use unlearned heuristics to combine features at the point level. To this end, we introduce the concept of the multi-view point cloud (Voint cloud), representing each 3D point as a set of features extracted from several view-points. This novel 3D Voint cloud representation combines the compactness of 3D point cloud representation with the natural view-awareness of multi-view representation. Naturally, we can equip this new representation with convolutional and pooling operations. We deploy a Voint neural network (VointNet) to learn representations in the Voint space. Our novel representation achieves state-of-the-art performance on 3D classification, shape retrieval, and robust 3D part segmentation on standard benchmarks ( ScanObjectNN, ShapeNet Core55, and ShapeNet Parts).","tags":["3D point cloud","3D classification","multi-view","3D retrieval","3D segmentation"],"title":"Voint Cloud: Multi-View Point Cloud Representation for 3D Understanding","type":"publication"},{"authors":["**Abdullah Hamdi**","Silvio Giancola","Bernard Ghanem"],"categories":null,"content":"ICCV 2021 received a record number of 6236 valid submissions, of which 1617 (25.9%) were accepted for publication.\n","date":1627160400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627160400,"objectID":"0ed87869cf5435e7561e43695f59b534","permalink":"https://abdullahamdi.com/publication/mvtn-iccv/","publishdate":"2021-07-25T00:00:00+03:00","relpermalink":"/publication/mvtn-iccv/","section":"publication","summary":"Multi-view projection methods have demonstrated their ability to reach state-of-the-art performance on 3D shape recognition. Those methods learn different ways to aggregate information from multiple views. However, the camera view-points for those views tend to be heuristically set and fixed for all shapes. To circumvent the lack of dynamism of current multi-view methods, we propose to learn those view-points. In particular, we introduce the Multi-View Transformation Network (MVTN) that regresses optimal view-points for 3D shape recognition, building upon advances in differentiable rendering. As a result, MVTN can be trained end-to-end along with any multi-view network for 3D shape classification. We integrate MVTN in a novel adaptive multi-view pipeline that can render either 3D meshes or point clouds. MVTN exhibits clear performance gains in the tasks of 3D shape classification and 3D shape retrieval without the need for extra training supervision. In these tasks, MVTN achieves state-of-the-art performance on ModelNet40, ShapeNet Core55, and the most recent and realistic ScanObjectNN dataset (up to 6% improvement). Interestingly, we also show that MVTN can provide network robustness against rotation and occlusion in the 3D domain.","tags":["3D point cloud","3D classification","multi-view","3D retrieval"],"title":"MVTN: Multi-View Transformation Network for 3D Shape Recognition","type":"publication"},{"authors":["Salman Alsubaihi","Adel Bibi","Modar Alfadly","**Abdullah Hamdi**","Bernard Ghanem"],"categories":null,"content":"","date":1613336400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613336400,"objectID":"3af6b04cd310b2bf80bc8f32c2d52f17","permalink":"https://abdullahamdi.com/publication/etb-iclr21/","publishdate":"2021-02-15T00:00:00+03:00","relpermalink":"/publication/etb-iclr21/","section":"publication","summary":"Training Deep Neural Networks (DNNs) that are robust to norm bounded adversarial attacks remains an elusive problem. While verification based methods are generally too expensive to robustly train large networks, it was demonstrated in Gowal et al that bounded input intervals can be inexpensively propagated per layer through large networks. This interval bound propagation (IBP) approach led to high robustness and was the first to be employed on large networks. However, due to the very loose nature of the IBP bounds, particularly for large networks, the required training procedure is complex and involved. In this paper, we closely examine the bounds of a block of layers composed of an affine layer followed by a ReLU nonlinearity followed by another affine layer. To this end, we propose expected bounds, true bounds in expectation, that are provably tighter than IBP bounds in expectation. We then extend this result to deeper networks through blockwise propagation and show that we can achieve orders of magnitudes tighter bounds compared to IBP. With such tight bounds, we demonstrate that a simple standard training procedure can achieve the best robustness-accuracy trade-off across several architectures on both MNIST and CIFAR10.","tags":["robustness","bounds propagation","adverserial attacks"],"title":"Expected Tight Bounds for Robust Training","type":"publication"},{"authors":["**Abdullah Hamdi**","Sara Rojas","Ali Thabet","Bernard Ghanem"],"categories":null,"content":"ECCV 2020 received a record number of 5025 valid submissions, of which 1361 (27%) were accepted for publication.\n","date":1598907600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598907600,"objectID":"53704510c5552ae0254e176b29e412b1","permalink":"https://abdullahamdi.com/publication/advpc-arxive/","publishdate":"2020-09-01T00:00:00+03:00","relpermalink":"/publication/advpc-arxive/","section":"publication","summary":"Deep neural networks are vulnerable to adversarial attacks, in which imperceptible perturbations to their input lead to erroneous network predictions. This phenomenon has been extensively studied in the image domain, and has only recently been extended to 3D point clouds. In this work, we present novel data-driven adversarial attacks against 3D point cloud networks. We aim to address the following problems in current 3D point cloud adversarial attacks: they do not transfer well between different networks, and they are easy to defend against via simple tatistical methods. To this extent, we develop a new point cloud attack (dubbed AdvPC) that exploits the input data distribution by adding an adversarial loss, after Auto-Encoder reconstruction, to the objective it optimizes. AdvPC leads to perturbations that are resilient against current defenses, while remaining highly transferable compared to state-of-the-art attacks. We test AdvPC using four popular point cloud networks: PointNet, PointNet++ (MSG and SSG), and DGCNN. Our proposed attack increases the attack success rate by up to 40% for those transferred to unseen networks (transferability), while maintaining a high success rate on the attacked network. AdvPC also increases the ability to break defenses by up to 38% as compared to other baselines on the ModelNet40 dataset.","tags":["3D point cloud","robustness","adversarial attacks"],"title":"AdvPC: Transferable Adversarial Perturbations on 3D Point Clouds","type":"publication"},{"authors":["**Abdullah Hamdi**","Bernard Ghanem"],"categories":null,"content":"","date":1597438800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597438800,"objectID":"5bb5828d436cc917b231f6ec23d10f15","permalink":"https://abdullahamdi.com/publication/sr-iccv19/","publishdate":"2020-08-15T00:00:00+03:00","relpermalink":"/publication/sr-iccv19/","section":"publication","summary":"Despite the impressive performance of Deep Neural Networks (DNNs) on various vision tasks, they still exhibit erroneous high sensitivity toward semantic primitives (e.g. object pose). We propose a theoretically grounded analysis for DNNs robustness in the semantic space. We qualitatively analyze different DNNs semantic robustness by visualizing the DNN global behavior as semantic maps and observe interesting behavior of some DNNs. Since generating these semantic maps does not scale well with the dimensionality of the semantic space, we develop a bottom-up approach to detect robust regions of DNNs. To achieve this, We formalize the problem of finding robust semantic regions of the network as optimization of integral bounds and develop expressions for update directions of the region bounds. We use our developed formulations to quantitatively evaluate the semantic robustness of different famous network architectures. We show through extensive experimentation that several networks, though trained on the same dataset and while enjoying comparable accuracy, they do not necessarily perform similarly in semantic robustness. For example, InceptionV3 is more accurate despite being less semantically robust than ResNet50. We hope that this tool will serve as the first milestone towards understanding the semantic robustness of DNNs.","tags":["semantic robustness","robustness","adverserial attacks"],"title":"Towards Analyzing Semantic Robustness of Deep Neural Networks","type":"publication"},{"authors":["**Abdullah Hamdi**","Matthias Muller","Bernard Ghanem"],"categories":null,"content":"In AAAI 2020, a record number of over 8,800 papers were submitted, 7,737 were reviewed, only 1,591 papers (20.6%) were accepted.\n","date":1572901200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572901200,"objectID":"07cd34a3c2a88d337be3375374d44a7d","permalink":"https://abdullahamdi.com/publication/sada-aaai20/","publishdate":"2019-11-05T00:00:00+03:00","relpermalink":"/publication/sada-aaai20/","section":"publication","summary":"One major factor impeding more widespread adoption of deep neural networks (DNNs) is their lack of robustness, which is essential for safety-critical applications such as autonomous driving. This has motivated much recent work on adversarial attacks for DNNs, which mostly focus on pixel-level perturbations void of semantic meaning. In contrast, we present a general framework for adversarial attacks on trained agents, which covers semantic perturbations to the environment of the agent performing the task as well as pixel-level attacks. To do this, we re-frame the adversarial attack problem as learning a distribution of parameters that always fools the agent. In the semantic case, our proposed adversary (denoted as BBGAN) is trained to sample parameters that describe the environment with which the black-box agent interacts, such that the agent performs its dedicated task poorly in this environment. We apply BBGAN on three different tasks, primarily targeting aspects of autonomous navigation: object detection, self-driving, and autonomous UAV racing. On these tasks, BBGAN can generate failure cases that consistently fool a trained agent.","tags":["sada","robustness","adverserial attacks"],"title":"SADA: Semantic Adversarial Diagnostic Attacks for Autonomous Applications","type":"publication"},{"authors":["**Abdullah Hamdi**","Bernard Ghanem"],"categories":null,"content":"","date":1555275600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555275600,"objectID":"daae23a478bedeb9fda9f8efbc5a55d2","permalink":"https://abdullahamdi.com/publication/ian-arxive/","publishdate":"2019-04-15T00:00:00+03:00","relpermalink":"/publication/ian-arxive/","section":"publication","summary":"Generative Adversarial Networks (GANs) have gained momentum for their ability to model image distributions. They learn to emulate the training set and that enables sampling from that domain and using the knowledge learned for useful applications. Several methods proposed enhancing GANs, including regularizing the loss with some feature matching. We seek to push GANs beyond the data in the training and try to explore unseen territory in the image manifold. We first propose a new regularizer for GAN based on K-nearest neighbor (K-NN) selective feature matching to a target set Y in high-level feature space, during the adversarial training of GAN on the base set X, and we call this novel model K-GAN. We show that minimizing the added term follows from cross-entropy minimization between the distributions of GAN and the set Y. Then, We introduce a cascaded framework for GANs that try to address the task of imagining a new distribution that combines the base set X and target set Y by cascading sampling GANs with translation GANs, and we dub the cascade of such GANs as the Imaginative Adversarial Network (IAN). We conduct an objective and subjective evaluation for different IAN setups in the addressed task and show some useful applications for these IANs, like manifold traversing and creative face generation for characters' design in movies or video games.","tags":["GANs","adversarial","generative"],"title":"IAN: Combining Generative Adversarial Networks for Imaginative Face Generation","type":"publication"},{"authors":["**Abdullah Hamdi**"],"categories":null,"content":"","date":1523221200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1523221200,"objectID":"2dd8c053d10eb96d158a1b8b7457527f","permalink":"https://abdullahamdi.com/publication/ms-thesis/","publishdate":"2018-04-09T00:00:00+03:00","relpermalink":"/publication/ms-thesis/","section":"publication","summary":"Abundance of labelled data played a crucial role in the recent developments in computer vision, but that faces problems like scalability and transferability to the wild. One alternative approach is to utilize the data without labels, i.e. unsupervised learning, in learning valuable information and put it in use to tackle vision problems. Generative Adversarial Networks (GANs) have gained momentum for their ability to model image distributions in unsupervised manner. They learn to emulate the training set and that enables sampling from that domain and using the knowledge learned for useful applications. Several methods proposed enhancing GANs, including regularizing the loss with some feature matching. We seek to push GANs beyond the data in the training and try to explore unseen territory in the image manifold. We first propose a new regularizer for GAN based on K-Nearest Neighbor (K-NN) selective feature matching to a target set Y in high-level feature space, during the adversarial training of GAN on the base set X, and we call this novel model K-GAN. We show that minimizing the added term follows from cross-entropy minimization between the distributions of GAN and set Y. Then, we introduce a cascaded framework for GANs that try to address the task of imagining a new distribution that combines the base set X and target set Y by cascading sampling GANs with translation GANs, and we dub the cascade of such GANs as the Imaginative Adversarial Network (IAN). Several cascades are trained on a collected dataset Zoo-Faces and generated innovative samples are shown, including from K-GAN cascade. We conduct an objective and subjective evaluation for different IAN setups in the addressed task of generating innovative samples and we show the effect of regularizing GAN on different scores. We conclude with some useful applications for these IANs, like multi-domain manifold traversing.","tags":["GAN","adverserial","Deep learning","Generative models"],"title":"Cascading Generative Adversarial Networks for Targeted Imagination","type":"publication"},{"authors":["**Abdullah Hamdi**"],"categories":null,"content":"","date":1519074000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519074000,"objectID":"ba6ee759e6ff8140614c5e3cd265b8b7","permalink":"https://abdullahamdi.com/publication/us-patent18/","publishdate":"2018-02-20T00:00:00+03:00","relpermalink":"/publication/us-patent18/","section":"publication","summary":"The smart dust-cleaner and cooler for solar photo-voltaic (PV) panels is a smooth transparent shield with low absorption coefficient (such as a plastic sheet) placed on top of the PV panel to facilitate removal of dust particulates. Two membrane vibrators (MVs) are placed on opposite sides of the PV panel. The vibrators have the ability to shake and resonate the transparent shield, dislodging the dust particulates from their positions. A compressor powered by the PV panel compresses air before a dust cleaning/cooling process, in which a short duration release of the compressed air creates an air stream over the PV panel that removes the loose dust particulates and cools the PV panel to enhance performance. Using a microcontroller-based timer, the dust cleaning/cooling process is timed for daily operation before noon, when the PV panel temperature is at its peak to maximize PV panel efficiency at maximum irradiance time.","tags":["patents","solar","solar tracker","PV panels"],"title":"Smart dust-cleaner and cooler for solar PV panels","type":"publication"},{"authors":["**Abdullah Hamdi**","Bernard Ghanem"],"categories":null,"content":"","date":1497128400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1497128400,"objectID":"e095b4301b43040534e4eff9dc5fc407","permalink":"https://abdullahamdi.com/publication/filter-arxive/","publishdate":"2017-06-11T00:00:00+03:00","relpermalink":"/publication/filter-arxive/","section":"publication","summary":"Kernel Correlation Filters have shown a very promising scheme for visual tracking in terms of speed and accuracy on several benchmarks. However it suffers from problems that affect its performance like occlusion, rotation and scale change. This paper tries to tackle the problem of rotation by reformulating the optimization problem for learning the correlation filter. This modification (RKCF) includes learning rotation filter that utilizes circulant structure of HOG feature to guesstimate rotation from one frame to another and enhance the detection of KCF. Hence it gains boost in overall accuracy in many of OBT50 detest videos with minimal additional computation.","tags":["KFC","tracking","UAV"],"title":"Learning Rotation for Kernel Correlation Filter","type":"publication"}]