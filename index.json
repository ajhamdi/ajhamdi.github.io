[{"authors":["admin"],"categories":null,"content":"I am a postdoctoral research fellow in machine learning and computer vision at the Visual Geomtry Group of the University of Oxford with Prof. Andrew Zisserman and Prof. Andrea Vedaldi. Prior to that, I earned PhD and MS degrees in Electrical an computer Engineering (Computer Vision) from KAUST, working on 3D understanding with deep neural networks. I was part of the Image and Video Understanding Laboratory (IVUL) in the Visual Computing Center (VCC), advised by Bernard Ghanem. I was a visiting PhD student with Prof Matthias Niessner at TUM in 2022 for 5 months in Munich. I received my my undergraduate degree form KFUPM in Electrical Engineering. My carear goal is to develop robust deep learning tools for 3D understanding and creation and to expand the access of AI to disadvantaged groups in the Arabic region. [video, article, TedX talk]. I am also the founder and president of fihm.ai (biggest Arabic online AI platform).\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://abdullahamdi.com/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a postdoctoral research fellow in machine learning and computer vision at the Visual Geomtry Group of the University of Oxford with Prof. Andrew Zisserman and Prof. Andrea Vedaldi. Prior to that, I earned PhD and MS degrees in Electrical an computer Engineering (Computer Vision) from KAUST, working on 3D understanding with deep neural networks. I was part of the Image and Video Understanding Laboratory (IVUL) in the Visual Computing Center (VCC), advised by Bernard Ghanem.","tags":null,"title":"Abdullah Hamdi","type":"authors"},{"authors":["Emmanuelle Bourigault*","**Abdullah Hamdi***","Amir Jamaludin"],"categories":null,"content":"","date":1713308400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1713308400,"objectID":"007838d145a7f271b9e076693d428f33","permalink":"https://abdullahamdi.com/publication/xdiff/","publishdate":"2024-04-17T00:00:00+01:00","relpermalink":"/publication/xdiff/","section":"publication","summary":"In this work, we present X-Diffusion, a cross-sectional diffusion model tailored for Magnetic Resonance Imaging (MRI) data. X-Diffusion is capable of generating the entire MRI volume from just a single MRI slice or optionally from few multiple slices, setting new benchmarks in the precision of synthesized MRIs from extremely sparse observations. The uniqueness lies in the novel view-conditional training and inference of X-Diffusion on MRI volumes, allowing for generalized MRI learning. Our evaluations span both brain tumour MRIs from the BRATS dataset and full-body MRIs from the UK Biobank dataset. Utilizing the paired pre-registered Dual-energy X-ray Absorptiometry (DXA) and MRI modalities in the UK Biobank dataset, X-Diffusion is able to generate detailed 3D MRI volume from a single full-body DXA. Remarkably, the resultant MRIs not only stand out in precision on unseen examples (surpassing state-of-the-art results by large margins) but also flawlessly retain essential features of the original MRI, including tumour profiles, spine curvature, brain volume, and beyond. Furthermore, the trained X-Diffusion model on the MRI datasets attains a generalization capacity out-of-domain (e.g. generating knee MRIs even though it is trained on brains).","tags":["Medical Imaing","MRI","multi-view","novel view synthesis","3D","Diffusion"],"title":"X-Diffusion: Generating Detailed 3D MRI Volumes From a Single Image Using Cross-Sectional Diffusion Models","type":"publication"},{"authors":["**Abdullah Hamdi**","Luke Melas-Kyriazi","Jinjie Mai","Guocheng Qian","Ruoshi Liu","Carl Vondrick","Bernard Ghanem","Andrea Vedaldi"],"categories":null,"content":"","date":1708128000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1708128000,"objectID":"5f64480129f744983090706227fc63a7","permalink":"https://abdullahamdi.com/publication/ges/","publishdate":"2024-02-17T00:00:00Z","relpermalink":"/publication/ges/","section":"publication","summary":"Advancements in 3D Gaussian Splatting (GS) have significantly accelerated 3D reconstruction and generation. However, it may require a very large number of Gaussians, which can become a substantial memory footprint. This paper introduces GES (Generalized Exponential Splatting), a novel representation that employs Generalized Exponential Function (GEF) to model 3D scenes, requiring far fewer particles to represent a scene and thus significantly outperforming Gaussian Splatting methods in efficiency with a plug-and-play replacement ability for Gaussian-based utilities. GES is validated theoretically and empirically in both principled 1D setup and realistic 3D contexts. It is shown to more accurately represent signals with sharp edges, which are typically challenging for Gaussians due to their inherent low-pass characteristics of the Gaussian function.Our empirical analysis demonstrates that GEF outperforms Gaussians in fitting natural-occurring signals (e.g. squares, triangles, parabolic signals), thereby reducing the need for extensive splitting operations that increase the memory footprint of Gaussian Splatting. With the aid of a frequency-modulated loss, GES achieves competitive performance in novel-view synthesis standard benchmarks while requiring less than half the memory storage of Gaussian Splatting.","tags":["3D point cloud","3D radiance fields","multi-view","novel view synthesis","NeRF","Gaussian Splatting"],"title":"GES: Generalized Exponential Splatting for Efficient Radiance Field Rendering","type":"publication"},{"authors":["Guocheng Qian","Jinjie Mai","**Abdullah Hamdi**","Jian Ren","Aliaksandr Siarohin","Bing Li","Hsin-Ying Lee","Ivan Skorokhodov","Peter Wonka","Sergey Tulyakov","Bernard Ghanem"],"categories":null,"content":"","date":1705276800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1705276800,"objectID":"54e04b3ae123e5b497f14f4bb91765d1","permalink":"https://abdullahamdi.com/publication/magic123/","publishdate":"2024-01-15T00:00:00Z","relpermalink":"/publication/magic123/","section":"publication","summary":"We present `Magic123`, a two-stage coarse-to-fine solution for high-quality, textured 3D meshes generation from a single unposed image in the wild using both 2D and 3D priors. In the first stage, we optimize a neural radiance field to produce a coarse geometry. In the second stage, we adopt a memory-efficient differentiable mesh representation to yield a high-resolution mesh with a visually appealing texture. In both stages, the 3D content is learned through reference view supervision and novel views guided by both 2D and 3D diffusion priors. We introduce a single tradeoff parameter between the 2D and 3D priors to control exploration (more imaginative) and exploitation (more precise) of the generated geometry. Additionally, We employ textual inversion and monocular depth regularization to encourage consistent appearances across views and to prevent degenerate solutions, respectively. Magic123 demonstrates a significant improvement over previous image-to-3D techniques, as validated through extensive experiments on synthetic benchmarks and diverse real-world images.","tags":["3D point cloud","3D radiance fields","multi-view","3D retrieval","NeRF","sparse voxels"],"title":"Magic123: One Image to High-Quality 3D Object Generation Using Both 2D and 3D Diffusion Priors","type":"publication"},{"authors":["Guocheng Qian","**Abdullah Hamdi**","Xingdi Zhang","Bernard Ghanem"],"categories":null,"content":"","date":1697410800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1697410800,"objectID":"a721d6e6bf14d4314fab1bbdda21ceb1","permalink":"https://abdullahamdi.com/publication/pix4point/","publishdate":"2023-10-16T00:00:00+01:00","relpermalink":"/publication/pix4point/","section":"publication","summary":"While Transformers have achieved impressive success in natural language processing and computer vision, their performance on 3D point clouds is relatively poor. This is mainly due to the limitation of Transformers: a demanding need for extensive training data. Unfortunately, in the realm of 3D point clouds, the availability of large datasets is a challenge, exacerbating the issue of training Transformers for 3D tasks. In this work, we solve the data issue of point cloud Transformers from two perspectives: (i) introducing more inductive bias to reduce the dependency of Transformers on data, and (ii) relying on cross-modality pretraining. More specifically, we first present Progressive Point Patch Embedding and present a new point cloud Transformer model namely PViT. PViT shares the same backbone as Transformer but is shown to be less hungry for data, enabling Transformer to achieve performance comparable to the state-of-the-art. Second, we formulate a simple yet effective pipeline dubbed Pix4Point that allows harnessing Transformers pretrained in the image domain to enhance downstream point cloud understanding. This is achieved through a modality-agnostic Transformer backbone with the help of a tokenizer and decoder specialized in the different domains. Pretrained on a large number of widely available images, significant gains of PViT are observed in the tasks of 3D point cloud classification, part segmentation, and semantic segmentation on ScanObjectNN, ShapeNetPart, and S3DIS, respectively.","tags":["3D point cloud","3D understanding","transformers","3D segmentation","3D classification"],"title":"Pix4Point: Image Pretrained Standard Transformers for 3D Point Cloud Understanding","type":"publication"},{"authors":["Jinjie Mai","**Abdullah Hamdi**","Silvio Giancola","Chen Zhao","Bernard Ghanem"],"categories":null,"content":"","date":1695596400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1695596400,"objectID":"aab30e84bbceb55b3b9cb2623a428238","permalink":"https://abdullahamdi.com/publication/egoloc-iccv/","publishdate":"2023-09-25T00:00:00+01:00","relpermalink":"/publication/egoloc-iccv/","section":"publication","summary":"With the recent advances in video and 3D understanding, novel 4D spatio-temporal methods fusing both concepts have emerged. Towards this direction, the Ego4D Episodic Memory Benchmark proposed a task for Visual Queries with 3D Localization (VQ3D). Given an egocentric video clip and an image crop depicting a query object, the goal is to localize the 3D position of the center of that query object with respect to the camera pose of a query frame. Current methods tackle the problem of VQ3D by unprojecting the 2D localization results of the sibling task Visual Queries with 2D Localization (VQ2D) into 3D predictions. Yet, we point out that the low number of camera poses caused by camera re-localization from previous VQ3D methods severally hinders their overall success rate. In this work, we formalize a pipeline (we dub EgoLoc) that better entangles 3D multiview geometry with 2D object retrieval from egocentric videos. Our approach involves estimating more robust camera poses and aggregating multi-view 3D displacements by leveraging the 2D detection confidence, which enhances the success rate of object queries and leads to a significant improvement in the VQ3D baseline performance. Specifically, our approach achieves an overall success rate of up to 87.12%, which sets a new state-of-the-art result in the VQ3D task. We provide a comprehensive empirical analysis of the VQ3D task and existing solutions and highlight the remaining challenges in VQ3D. The code and models will be released upon publication to set a new standard for the VQ3D task","tags":["3D point cloud","3D classification","multi-view","3D retrieval"],"title":"EgoLoc: Revisiting 3D Object Localization from Egocentric Videos with Visual Queries","type":"publication"},{"authors":["Jinjie Mai","**Abdullah Hamdi**","Silvio Giancola","Chen Zhao","Bernard Ghanem"],"categories":null,"content":"","date":1695596400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1695596400,"objectID":"42905b19681ee4cfbaeb71d0ed467ea2","permalink":"https://abdullahamdi.com/publication/track/","publishdate":"2023-09-25T00:00:00+01:00","relpermalink":"/publication/track/","section":"publication","summary":"Neural radiance fields (NeRFs) generally require many images with accurate poses for accurate novel view synthesis, which does not reflect realistic setups where views can be sparse and poses can be noisy. Previous solutions for learning NeRFs with sparse views and noisy poses only consider local geometry consistency with pairs of views. Closely following bundle adjustment in Structure-from-Motion (SfM), we introduce TrackNeRF for more globally consistent geometry reconstruction and more accurate pose optimization. TrackNeRF introduces feature tracks, i.e. connected pixel trajectories across all visible views that correspond to the same 3D points. By enforcing reprojection consistency among feature tracks, TrackNeRF encourages holistic 3D consistency explicitly. Through extensive experiments, TrackNeRF sets a new benchmark in noisy and sparse view reconstruction. In particular, TrackNeRF shows significant improvements over the state-of-the-art BARF and SPARF by ∼8 and ∼1 in terms of PSNR on DTU under various sparse and noisy view setups. ","tags":["3D point cloud","3D classification","multi-view","3D retrieval"],"title":"EgoLoc: Revisiting 3D Object Localization from Egocentric Videos with Visual Queries","type":"publication"},{"authors":["**Abdullah Hamdi**","Bernard Ghanem","Matthias Nießner"],"categories":null,"content":"","date":1692486000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1692486000,"objectID":"4b268dd6fee6ebf6a4b3e14cf350e123","permalink":"https://abdullahamdi.com/publication/sparf/","publishdate":"2023-08-20T00:00:00+01:00","relpermalink":"/publication/sparf/","section":"publication","summary":"Recent advances in Neural Radiance Fields (NeRFs) treat the problem of novel view synthesis as Sparse Radiance Field (SRF) optimization using sparse voxels for efficient and fast rendering (Plenoxels,InstantNGP). In order to leverage machine learning and adoption of SRFs as a 3D representation, we present SPARF, a large-scale ShapeNet-based synthetic dataset for novel view synthesis consisting of ~ 17 million images rendered from nearly 40,000 shapes at high resolution (400 X 400 pixels). The dataset is orders of magnitude larger than existing synthetic datasets for novel view synthesis and includes more than one million 3D-optimized radiance fields with multiple voxel resolutions. Furthermore, we propose a novel pipeline (SuRFNet) that learns to generate sparse voxel radiance fields from only few views. This is done by using the densely collected SPARF dataset and 3D sparse convolutions. SuRFNet employs partial SRFs from few/one images and a specialized SRF loss to learn to generate high-quality sparse voxel radiance fields that can be rendered from novel views. Our approach achieves state-of-the-art results in the task of unconstrained novel view synthesis based on few views on ShapeNet as compared to recent baselines.","tags":["3D point cloud","3D radiance fields","multi-view","3D retrieval","NeRF","sparse voxels"],"title":"SPARF: Large-Scale Learning of 3D Sparse Radiance Fields from Few Input Images","type":"publication"},{"authors":["Jan Held","Anthony Cioppa","Silvio Giancola","**Abdullah Hamdi**","Bernard Ghanem","Marc Van Droogenbroeck"],"categories":null,"content":"","date":1687388400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1687388400,"objectID":"957aed0b346e9060faf15db2d149b29b","permalink":"https://abdullahamdi.com/publication/vars-cvpr/","publishdate":"2023-06-22T00:00:00+01:00","relpermalink":"/publication/vars-cvpr/","section":"publication","summary":"The Video Assistant Referee (VAR) has revolutionized association football, enabling referees to review incidents on the pitch, make informed decisions, and ensure fairness. However, due to the lack of referees in many countries and the high cost of the VAR infrastructure, only professional leagues can benefit from it. In this paper, we propose a Video Assistant Referee System (VARS) that can automate soccer decision-making. VARS leverages the latest findings in multi-view video analysis, to provide real-time feedback to the referee, and help them make informed decisions that can impact the outcome of a game. To validate VARS, we introduce SoccerNet-MVFoul, a novel video dataset of soccer fouls from multiple camera views, annotated with extensive foul descriptions by a professional soccer referee, and we benchmark our VARS to automatically recognize the characteristics of these fouls. We believe that VARS has the potential to revolutionize soccer refereeing and take the game to new heights of fairness and accuracy across all levels of professional and amateur federations.","tags":["3D point cloud","3D radiance fields","multi-view","3D retrieval","NeRF","sparse voxels"],"title":"VARS: Video Assistant Referee System for Automated Soccer Decision Making From Multiple Views","type":"publication"},{"authors":["**Abdullah Hamdi**","Silvio Giancola","Bernard Ghanem"],"categories":null,"content":"","date":1664060400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664060400,"objectID":"50869c3251b905aeef93bce9f4fc1c33","permalink":"https://abdullahamdi.com/publication/voint-cloud/","publishdate":"2022-09-25T00:00:00+01:00","relpermalink":"/publication/voint-cloud/","section":"publication","summary":"Multi-view projection methods have demonstrated promising performance on 3D understanding tasks like 3D classification and segmentation. However, it remains unclear how to combine such multi-view methods with the widely available 3D point clouds. Previous methods use unlearned heuristics to combine features at the point level. To this end, we introduce the concept of the multi-view point cloud (Voint cloud), representing each 3D point as a set of features extracted from several view-points. This novel 3D Voint cloud representation combines the compactness of 3D point cloud representation with the natural view-awareness of multi-view representation. Naturally, we can equip this new representation with convolutional and pooling operations. We deploy a Voint neural network (VointNet) to learn representations in the Voint space. Our novel representation achieves state-of-the-art performance on 3D classification, shape retrieval, and robust 3D part segmentation on standard benchmarks ( ScanObjectNN, ShapeNet Core55, and ShapeNet Parts).","tags":["3D point cloud","3D classification","multi-view","3D retrieval","3D segmentation"],"title":"Voint Cloud: Multi-View Point Cloud Representation for 3D Understanding","type":"publication"},{"authors":["**Abdullah Hamdi**","Silvio Giancola","Bernard Ghanem"],"categories":null,"content":"ICCV 2021 received a record number of 6236 valid submissions, of which 1617 (25.9%) were accepted for publication.\n","date":1627167600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627167600,"objectID":"0ed87869cf5435e7561e43695f59b534","permalink":"https://abdullahamdi.com/publication/mvtn-iccv/","publishdate":"2021-07-25T00:00:00+01:00","relpermalink":"/publication/mvtn-iccv/","section":"publication","summary":"Multi-view projection methods have demonstrated their ability to reach state-of-the-art performance on 3D shape recognition. Those methods learn different ways to aggregate information from multiple views. However, the camera view-points for those views tend to be heuristically set and fixed for all shapes. To circumvent the lack of dynamism of current multi-view methods, we propose to learn those view-points. In particular, we introduce the Multi-View Transformation Network (MVTN) that regresses optimal view-points for 3D shape recognition, building upon advances in differentiable rendering. As a result, MVTN can be trained end-to-end along with any multi-view network for 3D shape classification. We integrate MVTN in a novel adaptive multi-view pipeline that can render either 3D meshes or point clouds. MVTN exhibits clear performance gains in the tasks of 3D shape classification and 3D shape retrieval without the need for extra training supervision. In these tasks, MVTN achieves state-of-the-art performance on ModelNet40, ShapeNet Core55, and the most recent and realistic ScanObjectNN dataset (up to 6% improvement). Interestingly, we also show that MVTN can provide network robustness against rotation and occlusion in the 3D domain.","tags":["3D point cloud","3D classification","multi-view","3D retrieval"],"title":"MVTN: Multi-View Transformation Network for 3D Shape Recognition","type":"publication"},{"authors":["Salman Alsubaihi","Adel Bibi","Modar Alfadly","**Abdullah Hamdi**","Bernard Ghanem"],"categories":null,"content":"","date":1613347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613347200,"objectID":"3af6b04cd310b2bf80bc8f32c2d52f17","permalink":"https://abdullahamdi.com/publication/etb-iclr21/","publishdate":"2021-02-15T00:00:00Z","relpermalink":"/publication/etb-iclr21/","section":"publication","summary":"Training Deep Neural Networks (DNNs) that are robust to norm bounded adversarial attacks remains an elusive problem. While verification based methods are generally too expensive to robustly train large networks, it was demonstrated in Gowal et al that bounded input intervals can be inexpensively propagated per layer through large networks. This interval bound propagation (IBP) approach led to high robustness and was the first to be employed on large networks. However, due to the very loose nature of the IBP bounds, particularly for large networks, the required training procedure is complex and involved. In this paper, we closely examine the bounds of a block of layers composed of an affine layer followed by a ReLU nonlinearity followed by another affine layer. To this end, we propose expected bounds, true bounds in expectation, that are provably tighter than IBP bounds in expectation. We then extend this result to deeper networks through blockwise propagation and show that we can achieve orders of magnitudes tighter bounds compared to IBP. With such tight bounds, we demonstrate that a simple standard training procedure can achieve the best robustness-accuracy trade-off across several architectures on both MNIST and CIFAR10.","tags":["robustness","bounds propagation","adverserial attacks"],"title":"Expected Tight Bounds for Robust Training","type":"publication"},{"authors":["**Abdullah Hamdi**","Sara Rojas","Ali Thabet","Bernard Ghanem"],"categories":null,"content":"ECCV 2020 received a record number of 5025 valid submissions, of which 1361 (27%) were accepted for publication.\n","date":1598914800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598914800,"objectID":"53704510c5552ae0254e176b29e412b1","permalink":"https://abdullahamdi.com/publication/advpc-arxive/","publishdate":"2020-09-01T00:00:00+01:00","relpermalink":"/publication/advpc-arxive/","section":"publication","summary":"Deep neural networks are vulnerable to adversarial attacks, in which imperceptible perturbations to their input lead to erroneous network predictions. This phenomenon has been extensively studied in the image domain, and has only recently been extended to 3D point clouds. In this work, we present novel data-driven adversarial attacks against 3D point cloud networks. We aim to address the following problems in current 3D point cloud adversarial attacks: they do not transfer well between different networks, and they are easy to defend against via simple tatistical methods. To this extent, we develop a new point cloud attack (dubbed AdvPC) that exploits the input data distribution by adding an adversarial loss, after Auto-Encoder reconstruction, to the objective it optimizes. AdvPC leads to perturbations that are resilient against current defenses, while remaining highly transferable compared to state-of-the-art attacks. We test AdvPC using four popular point cloud networks: PointNet, PointNet++ (MSG and SSG), and DGCNN. Our proposed attack increases the attack success rate by up to 40% for those transferred to unseen networks (transferability), while maintaining a high success rate on the attacked network. AdvPC also increases the ability to break defenses by up to 38% as compared to other baselines on the ModelNet40 dataset.","tags":["3D point cloud","robustness","adversarial attacks"],"title":"AdvPC: Transferable Adversarial Perturbations on 3D Point Clouds","type":"publication"},{"authors":["**Abdullah Hamdi**","Bernard Ghanem"],"categories":null,"content":"","date":1597446000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597446000,"objectID":"5bb5828d436cc917b231f6ec23d10f15","permalink":"https://abdullahamdi.com/publication/sr-iccv19/","publishdate":"2020-08-15T00:00:00+01:00","relpermalink":"/publication/sr-iccv19/","section":"publication","summary":"Despite the impressive performance of Deep Neural Networks (DNNs) on various vision tasks, they still exhibit erroneous high sensitivity toward semantic primitives (e.g. object pose). We propose a theoretically grounded analysis for DNNs robustness in the semantic space. We qualitatively analyze different DNNs semantic robustness by visualizing the DNN global behavior as semantic maps and observe interesting behavior of some DNNs. Since generating these semantic maps does not scale well with the dimensionality of the semantic space, we develop a bottom-up approach to detect robust regions of DNNs. To achieve this, We formalize the problem of finding robust semantic regions of the network as optimization of integral bounds and develop expressions for update directions of the region bounds. We use our developed formulations to quantitatively evaluate the semantic robustness of different famous network architectures. We show through extensive experimentation that several networks, though trained on the same dataset and while enjoying comparable accuracy, they do not necessarily perform similarly in semantic robustness. For example, InceptionV3 is more accurate despite being less semantically robust than ResNet50. We hope that this tool will serve as the first milestone towards understanding the semantic robustness of DNNs.","tags":["semantic robustness","robustness","adverserial attacks"],"title":"Towards Analyzing Semantic Robustness of Deep Neural Networks","type":"publication"},{"authors":["**Abdullah Hamdi**","Matthias Muller","Bernard Ghanem"],"categories":null,"content":"In AAAI 2020, a record number of over 8,800 papers were submitted, 7,737 were reviewed, only 1,591 papers (20.6%) were accepted.\n","date":1572912000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572912000,"objectID":"07cd34a3c2a88d337be3375374d44a7d","permalink":"https://abdullahamdi.com/publication/sada-aaai20/","publishdate":"2019-11-05T00:00:00Z","relpermalink":"/publication/sada-aaai20/","section":"publication","summary":"One major factor impeding more widespread adoption of deep neural networks (DNNs) is their lack of robustness, which is essential for safety-critical applications such as autonomous driving. This has motivated much recent work on adversarial attacks for DNNs, which mostly focus on pixel-level perturbations void of semantic meaning. In contrast, we present a general framework for adversarial attacks on trained agents, which covers semantic perturbations to the environment of the agent performing the task as well as pixel-level attacks. To do this, we re-frame the adversarial attack problem as learning a distribution of parameters that always fools the agent. In the semantic case, our proposed adversary (denoted as BBGAN) is trained to sample parameters that describe the environment with which the black-box agent interacts, such that the agent performs its dedicated task poorly in this environment. We apply BBGAN on three different tasks, primarily targeting aspects of autonomous navigation: object detection, self-driving, and autonomous UAV racing. On these tasks, BBGAN can generate failure cases that consistently fool a trained agent.","tags":["sada","robustness","adverserial attacks"],"title":"SADA: Semantic Adversarial Diagnostic Attacks for Autonomous Applications","type":"publication"},{"authors":["**Abdullah Hamdi**","Bernard Ghanem"],"categories":null,"content":"","date":1555282800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555282800,"objectID":"daae23a478bedeb9fda9f8efbc5a55d2","permalink":"https://abdullahamdi.com/publication/ian-arxive/","publishdate":"2019-04-15T00:00:00+01:00","relpermalink":"/publication/ian-arxive/","section":"publication","summary":"Generative Adversarial Networks (GANs) have gained momentum for their ability to model image distributions. They learn to emulate the training set and that enables sampling from that domain and using the knowledge learned for useful applications. Several methods proposed enhancing GANs, including regularizing the loss with some feature matching. We seek to push GANs beyond the data in the training and try to explore unseen territory in the image manifold. We first propose a new regularizer for GAN based on K-nearest neighbor (K-NN) selective feature matching to a target set Y in high-level feature space, during the adversarial training of GAN on the base set X, and we call this novel model K-GAN. We show that minimizing the added term follows from cross-entropy minimization between the distributions of GAN and the set Y. Then, We introduce a cascaded framework for GANs that try to address the task of imagining a new distribution that combines the base set X and target set Y by cascading sampling GANs with translation GANs, and we dub the cascade of such GANs as the Imaginative Adversarial Network (IAN). We conduct an objective and subjective evaluation for different IAN setups in the addressed task and show some useful applications for these IANs, like manifold traversing and creative face generation for characters' design in movies or video games.","tags":["GANs","adversarial","generative"],"title":"IAN: Combining Generative Adversarial Networks for Imaginative Face Generation","type":"publication"},{"authors":["**Abdullah Hamdi**"],"categories":null,"content":"","date":1523228400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1523228400,"objectID":"2dd8c053d10eb96d158a1b8b7457527f","permalink":"https://abdullahamdi.com/publication/ms-thesis/","publishdate":"2018-04-09T00:00:00+01:00","relpermalink":"/publication/ms-thesis/","section":"publication","summary":"Abundance of labelled data played a crucial role in the recent developments in computer vision, but that faces problems like scalability and transferability to the wild. One alternative approach is to utilize the data without labels, i.e. unsupervised learning, in learning valuable information and put it in use to tackle vision problems. Generative Adversarial Networks (GANs) have gained momentum for their ability to model image distributions in unsupervised manner. They learn to emulate the training set and that enables sampling from that domain and using the knowledge learned for useful applications. Several methods proposed enhancing GANs, including regularizing the loss with some feature matching. We seek to push GANs beyond the data in the training and try to explore unseen territory in the image manifold. We first propose a new regularizer for GAN based on K-Nearest Neighbor (K-NN) selective feature matching to a target set Y in high-level feature space, during the adversarial training of GAN on the base set X, and we call this novel model K-GAN. We show that minimizing the added term follows from cross-entropy minimization between the distributions of GAN and set Y. Then, we introduce a cascaded framework for GANs that try to address the task of imagining a new distribution that combines the base set X and target set Y by cascading sampling GANs with translation GANs, and we dub the cascade of such GANs as the Imaginative Adversarial Network (IAN). Several cascades are trained on a collected dataset Zoo-Faces and generated innovative samples are shown, including from K-GAN cascade. We conduct an objective and subjective evaluation for different IAN setups in the addressed task of generating innovative samples and we show the effect of regularizing GAN on different scores. We conclude with some useful applications for these IANs, like multi-domain manifold traversing.","tags":["GAN","adverserial","Deep learning","Generative models"],"title":"Cascading Generative Adversarial Networks for Targeted Imagination","type":"publication"},{"authors":["**Abdullah Hamdi**"],"categories":null,"content":"","date":1519084800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519084800,"objectID":"ba6ee759e6ff8140614c5e3cd265b8b7","permalink":"https://abdullahamdi.com/publication/us-patent18/","publishdate":"2018-02-20T00:00:00Z","relpermalink":"/publication/us-patent18/","section":"publication","summary":"The smart dust-cleaner and cooler for solar photo-voltaic (PV) panels is a smooth transparent shield with low absorption coefficient (such as a plastic sheet) placed on top of the PV panel to facilitate removal of dust particulates. Two membrane vibrators (MVs) are placed on opposite sides of the PV panel. The vibrators have the ability to shake and resonate the transparent shield, dislodging the dust particulates from their positions. A compressor powered by the PV panel compresses air before a dust cleaning/cooling process, in which a short duration release of the compressed air creates an air stream over the PV panel that removes the loose dust particulates and cools the PV panel to enhance performance. Using a microcontroller-based timer, the dust cleaning/cooling process is timed for daily operation before noon, when the PV panel temperature is at its peak to maximize PV panel efficiency at maximum irradiance time.","tags":["patents","solar","solar tracker","PV panels"],"title":"Smart dust-cleaner and cooler for solar PV panels","type":"publication"},{"authors":["**Abdullah Hamdi**","Bernard Ghanem"],"categories":null,"content":"","date":1497135600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1497135600,"objectID":"e095b4301b43040534e4eff9dc5fc407","permalink":"https://abdullahamdi.com/publication/filter-arxive/","publishdate":"2017-06-11T00:00:00+01:00","relpermalink":"/publication/filter-arxive/","section":"publication","summary":"Kernel Correlation Filters have shown a very promising scheme for visual tracking in terms of speed and accuracy on several benchmarks. However it suffers from problems that affect its performance like occlusion, rotation and scale change. This paper tries to tackle the problem of rotation by reformulating the optimization problem for learning the correlation filter. This modification (RKCF) includes learning rotation filter that utilizes circulant structure of HOG feature to guesstimate rotation from one frame to another and enhance the detection of KCF. Hence it gains boost in overall accuracy in many of OBT50 detest videos with minimal additional computation.","tags":["KFC","tracking","UAV"],"title":"Learning Rotation for Kernel Correlation Filter","type":"publication"}]