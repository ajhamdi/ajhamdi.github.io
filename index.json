[{"authors":["admin"],"categories":null,"content":"I’m a PhD student at KAUST, pursuing to advance artificial intelligence via understanding deep neural networks. I am part of the Image and Video Understanding Laboratory (IVUL) in the Visual Computing Center (VCC), advised by Bernard Ghanem. I was a visiting PhD with Prof Matthias Niessner at TUM in 2022. I received my MSc degree in Electrical Engineering (Computer Vision) from KAUST, and my undergraduate degree form KFUPM in Electrical Engineering. My carear goal is to develop robust deep learning tools for 3D understanding and creation and to expand the access of AI to disadvantaged groups in the Arabic region. [video, article, TedX talk]. I am also the founder and president of fihm.ai (biggest Arabic online AI platform).\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://abdullahamdi.com/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I’m a PhD student at KAUST, pursuing to advance artificial intelligence via understanding deep neural networks. I am part of the Image and Video Understanding Laboratory (IVUL) in the Visual Computing Center (VCC), advised by Bernard Ghanem. I was a visiting PhD with Prof Matthias Niessner at TUM in 2022. I received my MSc degree in Electrical Engineering (Computer Vision) from KAUST, and my undergraduate degree form KFUPM in Electrical Engineering.","tags":null,"title":"Abdullah Hamdi","type":"authors"},{"authors":["**Abdullah Hamdi**","Bernard Ghanem","Matthias Nießner"],"categories":null,"content":"","date":1671483600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1671483600,"objectID":"4b268dd6fee6ebf6a4b3e14cf350e123","permalink":"https://abdullahamdi.com/publication/sparf/","publishdate":"2022-12-20T00:00:00+03:00","relpermalink":"/publication/sparf/","section":"publication","summary":"Recent advances in Neural Radiance Fields (NeRFs) treat the problem of novel view synthesis as Sparse Radiance Field (SRF) optimization using sparse voxels for efficient and fast rendering (Plenoxels,InstantNGP). In order to leverage machine learning and adoption of SRFs as a 3D representation, we present SPARF, a large-scale ShapeNet-based synthetic dataset for novel view synthesis consisting of ~ 17 million images rendered from nearly 40,000 shapes at high resolution (400 X 400 pixels). The dataset is orders of magnitude larger than existing synthetic datasets for novel view synthesis and includes more than one million 3D-optimized radiance fields with multiple voxel resolutions. Furthermore, we propose a novel pipeline (SuRFNet) that learns to generate sparse voxel radiance fields from only few views. This is done by using the densely collected SPARF dataset and 3D sparse convolutions. SuRFNet employs partial SRFs from few/one images and a specialized SRF loss to learn to generate high-quality sparse voxel radiance fields that can be rendered from novel views. Our approach achieves state-of-the-art results in the task of unconstrained novel view synthesis based on few views on ShapeNet as compared to recent baselines.","tags":["3D point cloud","3D radiance fields","multi-view","3D retrieval","NeRF","sparse voxels"],"title":"SPARF: Large-Scale Learning of 3D Sparse Radiance Fields from Few Input Images","type":"publication"},{"authors":["**Abdullah Hamdi**","Silvio Giancola","Bernard Ghanem"],"categories":null,"content":"","date":1664053200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664053200,"objectID":"50869c3251b905aeef93bce9f4fc1c33","permalink":"https://abdullahamdi.com/publication/voint-cloud/","publishdate":"2022-09-25T00:00:00+03:00","relpermalink":"/publication/voint-cloud/","section":"publication","summary":"Multi-view projection methods have demonstrated promising performance on 3D understanding tasks like 3D classification and segmentation. However, it remains unclear how to combine such multi-view methods with the widely available 3D point clouds. Previous methods use unlearned heuristics to combine features at the point level. To this end, we introduce the concept of the multi-view point cloud (Voint cloud), representing each 3D point as a set of features extracted from several view-points. This novel 3D Voint cloud representation combines the compactness of 3D point cloud representation with the natural view-awareness of multi-view representation. Naturally, we can equip this new representation with convolutional and pooling operations. We deploy a Voint neural network (VointNet) to learn representations in the Voint space. Our novel representation achieves state-of-the-art performance on 3D classification, shape retrieval, and robust 3D part segmentation on standard benchmarks ( ScanObjectNN, ShapeNet Core55, and ShapeNet Parts).","tags":["3D point cloud","3D classification","multi-view","3D retrieval","3D segmentation"],"title":"Voint Cloud: Multi-View Point Cloud Representation for 3D Understanding","type":"publication"},{"authors":["**Abdullah Hamdi**","Silvio Giancola","Bernard Ghanem"],"categories":null,"content":"ICCV 2021 received a record number of 6236 valid submissions, of which 1617 (25.9%) were accepted for publication.\n","date":1627160400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627160400,"objectID":"0ed87869cf5435e7561e43695f59b534","permalink":"https://abdullahamdi.com/publication/mvtn-iccv/","publishdate":"2021-07-25T00:00:00+03:00","relpermalink":"/publication/mvtn-iccv/","section":"publication","summary":"Multi-view projection methods have demonstrated their ability to reach state-of-the-art performance on 3D shape recognition. Those methods learn different ways to aggregate information from multiple views. However, the camera view-points for those views tend to be heuristically set and fixed for all shapes. To circumvent the lack of dynamism of current multi-view methods, we propose to learn those view-points. In particular, we introduce the Multi-View Transformation Network (MVTN) that regresses optimal view-points for 3D shape recognition, building upon advances in differentiable rendering. As a result, MVTN can be trained end-to-end along with any multi-view network for 3D shape classification. We integrate MVTN in a novel adaptive multi-view pipeline that can render either 3D meshes or point clouds. MVTN exhibits clear performance gains in the tasks of 3D shape classification and 3D shape retrieval without the need for extra training supervision. In these tasks, MVTN achieves state-of-the-art performance on ModelNet40, ShapeNet Core55, and the most recent and realistic ScanObjectNN dataset (up to 6% improvement). Interestingly, we also show that MVTN can provide network robustness against rotation and occlusion in the 3D domain.","tags":["3D point cloud","3D classification","multi-view","3D retrieval"],"title":"MVTN: Multi-View Transformation Network for 3D Shape Recognition","type":"publication"},{"authors":["Salman Alsubaihi","Adel Bibi","Modar Alfadly","**Abdullah Hamdi**","Bernard Ghanem"],"categories":null,"content":"","date":1613336400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613336400,"objectID":"3af6b04cd310b2bf80bc8f32c2d52f17","permalink":"https://abdullahamdi.com/publication/etb-iclr21/","publishdate":"2021-02-15T00:00:00+03:00","relpermalink":"/publication/etb-iclr21/","section":"publication","summary":"Training Deep Neural Networks (DNNs) that are robust to norm bounded adversarial attacks remains an elusive problem. While verification based methods are generally too expensive to robustly train large networks, it was demonstrated in Gowal et al that bounded input intervals can be inexpensively propagated per layer through large networks. This interval bound propagation (IBP) approach led to high robustness and was the first to be employed on large networks. However, due to the very loose nature of the IBP bounds, particularly for large networks, the required training procedure is complex and involved. In this paper, we closely examine the bounds of a block of layers composed of an affine layer followed by a ReLU nonlinearity followed by another affine layer. To this end, we propose expected bounds, true bounds in expectation, that are provably tighter than IBP bounds in expectation. We then extend this result to deeper networks through blockwise propagation and show that we can achieve orders of magnitudes tighter bounds compared to IBP. With such tight bounds, we demonstrate that a simple standard training procedure can achieve the best robustness-accuracy trade-off across several architectures on both MNIST and CIFAR10.","tags":["robustness","bounds propagation","adverserial attacks"],"title":"Expected Tight Bounds for Robust Training","type":"publication"},{"authors":["**Abdullah Hamdi**","Sara Rojas","Ali Thabet","Bernard Ghanem"],"categories":null,"content":"ECCV 2020 received a record number of 5025 valid submissions, of which 1361 (27%) were accepted for publication.\n","date":1598907600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598907600,"objectID":"53704510c5552ae0254e176b29e412b1","permalink":"https://abdullahamdi.com/publication/advpc-arxive/","publishdate":"2020-09-01T00:00:00+03:00","relpermalink":"/publication/advpc-arxive/","section":"publication","summary":"Deep neural networks are vulnerable to adversarial attacks, in which imperceptible perturbations to their input lead to erroneous network predictions. This phenomenon has been extensively studied in the image domain, and has only recently been extended to 3D point clouds. In this work, we present novel data-driven adversarial attacks against 3D point cloud networks. We aim to address the following problems in current 3D point cloud adversarial attacks: they do not transfer well between different networks, and they are easy to defend against via simple tatistical methods. To this extent, we develop a new point cloud attack (dubbed AdvPC) that exploits the input data distribution by adding an adversarial loss, after Auto-Encoder reconstruction, to the objective it optimizes. AdvPC leads to perturbations that are resilient against current defenses, while remaining highly transferable compared to state-of-the-art attacks. We test AdvPC using four popular point cloud networks: PointNet, PointNet++ (MSG and SSG), and DGCNN. Our proposed attack increases the attack success rate by up to 40% for those transferred to unseen networks (transferability), while maintaining a high success rate on the attacked network. AdvPC also increases the ability to break defenses by up to 38% as compared to other baselines on the ModelNet40 dataset.","tags":["3D point cloud","robustness","adversarial attacks"],"title":"AdvPC: Transferable Adversarial Perturbations on 3D Point Clouds","type":"publication"},{"authors":["**Abdullah Hamdi**","Bernard Ghanem"],"categories":null,"content":"","date":1597438800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597438800,"objectID":"5bb5828d436cc917b231f6ec23d10f15","permalink":"https://abdullahamdi.com/publication/sr-iccv19/","publishdate":"2020-08-15T00:00:00+03:00","relpermalink":"/publication/sr-iccv19/","section":"publication","summary":"Despite the impressive performance of Deep Neural Networks (DNNs) on various vision tasks, they still exhibit erroneous high sensitivity toward semantic primitives (e.g. object pose). We propose a theoretically grounded analysis for DNNs robustness in the semantic space. We qualitatively analyze different DNNs semantic robustness by visualizing the DNN global behavior as semantic maps and observe interesting behavior of some DNNs. Since generating these semantic maps does not scale well with the dimensionality of the semantic space, we develop a bottom-up approach to detect robust regions of DNNs. To achieve this, We formalize the problem of finding robust semantic regions of the network as optimization of integral bounds and develop expressions for update directions of the region bounds. We use our developed formulations to quantitatively evaluate the semantic robustness of different famous network architectures. We show through extensive experimentation that several networks, though trained on the same dataset and while enjoying comparable accuracy, they do not necessarily perform similarly in semantic robustness. For example, InceptionV3 is more accurate despite being less semantically robust than ResNet50. We hope that this tool will serve as the first milestone towards understanding the semantic robustness of DNNs.","tags":["semantic robustness","robustness","adverserial attacks"],"title":"Towards Analyzing Semantic Robustness of Deep Neural Networks","type":"publication"},{"authors":["**Abdullah Hamdi**","Matthias Muller","Bernard Ghanem"],"categories":null,"content":"In AAAI 2020, a record number of over 8,800 papers were submitted, 7,737 were reviewed, only 1,591 papers (20.6%) were accepted.\n","date":1572901200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572901200,"objectID":"07cd34a3c2a88d337be3375374d44a7d","permalink":"https://abdullahamdi.com/publication/sada-aaai20/","publishdate":"2019-11-05T00:00:00+03:00","relpermalink":"/publication/sada-aaai20/","section":"publication","summary":"One major factor impeding more widespread adoption of deep neural networks (DNNs) is their lack of robustness, which is essential for safety-critical applications such as autonomous driving. This has motivated much recent work on adversarial attacks for DNNs, which mostly focus on pixel-level perturbations void of semantic meaning. In contrast, we present a general framework for adversarial attacks on trained agents, which covers semantic perturbations to the environment of the agent performing the task as well as pixel-level attacks. To do this, we re-frame the adversarial attack problem as learning a distribution of parameters that always fools the agent. In the semantic case, our proposed adversary (denoted as BBGAN) is trained to sample parameters that describe the environment with which the black-box agent interacts, such that the agent performs its dedicated task poorly in this environment. We apply BBGAN on three different tasks, primarily targeting aspects of autonomous navigation: object detection, self-driving, and autonomous UAV racing. On these tasks, BBGAN can generate failure cases that consistently fool a trained agent.","tags":["sada","robustness","adverserial attacks"],"title":"SADA: Semantic Adversarial Diagnostic Attacks for Autonomous Applications","type":"publication"},{"authors":["**Abdullah Hamdi**","Bernard Ghanem"],"categories":null,"content":"","date":1555275600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555275600,"objectID":"daae23a478bedeb9fda9f8efbc5a55d2","permalink":"https://abdullahamdi.com/publication/ian-arxive/","publishdate":"2019-04-15T00:00:00+03:00","relpermalink":"/publication/ian-arxive/","section":"publication","summary":"Generative Adversarial Networks (GANs) have gained momentum for their ability to model image distributions. They learn to emulate the training set and that enables sampling from that domain and using the knowledge learned for useful applications. Several methods proposed enhancing GANs, including regularizing the loss with some feature matching. We seek to push GANs beyond the data in the training and try to explore unseen territory in the image manifold. We first propose a new regularizer for GAN based on K-nearest neighbor (K-NN) selective feature matching to a target set Y in high-level feature space, during the adversarial training of GAN on the base set X, and we call this novel model K-GAN. We show that minimizing the added term follows from cross-entropy minimization between the distributions of GAN and the set Y. Then, We introduce a cascaded framework for GANs that try to address the task of imagining a new distribution that combines the base set X and target set Y by cascading sampling GANs with translation GANs, and we dub the cascade of such GANs as the Imaginative Adversarial Network (IAN). We conduct an objective and subjective evaluation for different IAN setups in the addressed task and show some useful applications for these IANs, like manifold traversing and creative face generation for characters' design in movies or video games.","tags":["GANs","adversarial","generative"],"title":"IAN: Combining Generative Adversarial Networks for Imaginative Face Generation","type":"publication"},{"authors":["**Abdullah Hamdi**"],"categories":null,"content":"","date":1523221200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1523221200,"objectID":"2dd8c053d10eb96d158a1b8b7457527f","permalink":"https://abdullahamdi.com/publication/ms-thesis/","publishdate":"2018-04-09T00:00:00+03:00","relpermalink":"/publication/ms-thesis/","section":"publication","summary":"Abundance of labelled data played a crucial role in the recent developments in computer vision, but that faces problems like scalability and transferability to the wild. One alternative approach is to utilize the data without labels, i.e. unsupervised learning, in learning valuable information and put it in use to tackle vision problems. Generative Adversarial Networks (GANs) have gained momentum for their ability to model image distributions in unsupervised manner. They learn to emulate the training set and that enables sampling from that domain and using the knowledge learned for useful applications. Several methods proposed enhancing GANs, including regularizing the loss with some feature matching. We seek to push GANs beyond the data in the training and try to explore unseen territory in the image manifold. We first propose a new regularizer for GAN based on K-Nearest Neighbor (K-NN) selective feature matching to a target set Y in high-level feature space, during the adversarial training of GAN on the base set X, and we call this novel model K-GAN. We show that minimizing the added term follows from cross-entropy minimization between the distributions of GAN and set Y. Then, we introduce a cascaded framework for GANs that try to address the task of imagining a new distribution that combines the base set X and target set Y by cascading sampling GANs with translation GANs, and we dub the cascade of such GANs as the Imaginative Adversarial Network (IAN). Several cascades are trained on a collected dataset Zoo-Faces and generated innovative samples are shown, including from K-GAN cascade. We conduct an objective and subjective evaluation for different IAN setups in the addressed task of generating innovative samples and we show the effect of regularizing GAN on different scores. We conclude with some useful applications for these IANs, like multi-domain manifold traversing.","tags":["GAN","adverserial","Deep learning","Generative models"],"title":"Cascading Generative Adversarial Networks for Targeted Imagination","type":"publication"},{"authors":["**Abdullah Hamdi**"],"categories":null,"content":"","date":1519074000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519074000,"objectID":"ba6ee759e6ff8140614c5e3cd265b8b7","permalink":"https://abdullahamdi.com/publication/us-patent18/","publishdate":"2018-02-20T00:00:00+03:00","relpermalink":"/publication/us-patent18/","section":"publication","summary":"The smart dust-cleaner and cooler for solar photo-voltaic (PV) panels is a smooth transparent shield with low absorption coefficient (such as a plastic sheet) placed on top of the PV panel to facilitate removal of dust particulates. Two membrane vibrators (MVs) are placed on opposite sides of the PV panel. The vibrators have the ability to shake and resonate the transparent shield, dislodging the dust particulates from their positions. A compressor powered by the PV panel compresses air before a dust cleaning/cooling process, in which a short duration release of the compressed air creates an air stream over the PV panel that removes the loose dust particulates and cools the PV panel to enhance performance. Using a microcontroller-based timer, the dust cleaning/cooling process is timed for daily operation before noon, when the PV panel temperature is at its peak to maximize PV panel efficiency at maximum irradiance time.","tags":["patents","solar","solar tracker","PV panels"],"title":"Smart dust-cleaner and cooler for solar PV panels","type":"publication"},{"authors":["**Abdullah Hamdi**","Bernard Ghanem"],"categories":null,"content":"","date":1497128400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1497128400,"objectID":"e095b4301b43040534e4eff9dc5fc407","permalink":"https://abdullahamdi.com/publication/filter-arxive/","publishdate":"2017-06-11T00:00:00+03:00","relpermalink":"/publication/filter-arxive/","section":"publication","summary":"Kernel Correlation Filters have shown a very promising scheme for visual tracking in terms of speed and accuracy on several benchmarks. However it suffers from problems that affect its performance like occlusion, rotation and scale change. This paper tries to tackle the problem of rotation by reformulating the optimization problem for learning the correlation filter. This modification (RKCF) includes learning rotation filter that utilizes circulant structure of HOG feature to guesstimate rotation from one frame to another and enhance the detection of KCF. Hence it gains boost in overall accuracy in many of OBT50 detest videos with minimal additional computation.","tags":["KFC","tracking","UAV"],"title":"Learning Rotation for Kernel Correlation Filter","type":"publication"}]